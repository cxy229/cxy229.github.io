---
title: cs231n Neural Networks Part 1-Setting up the Architecture 中文翻译
date: 2018-05-02
link: http://70b86a48.wiz03.com/share/s/1MK6F81-vQ1i2DFlsT0ux-iU1HULyn1wAAdx2anequ1uqhwL
categories: translation
tags: 
  - Deep Learning
---

文章作者：cuizixin
**声明：作者翻译文章仅为学习，如有侵权请联系作者删除博文，谢谢！**

>翻译自[http://cs231n.github.io/neural-networks-1/](http://cs231n.github.io/neural-networks-1/)

# Neural Networks Part 1: Setting up the Architecture
**model of a biological neuron, activation functions, neural net architecture, representational power**

## 快速介绍
介绍神经网络时，不用大脑做类比是有可能的。 在关于线性分类（linear classification）的章节中，我们使用公式$s = Wx$计算不同图片类别的分数，其中$W$是矩阵，$x$是包含图像所有像素数据的输入列向量。 在CIFAR-10的情况下，$x$是[3072x1]列向量，$W$是[10x3072]矩阵，因此输出分数是有10个分数的向量。

然而一个神经网络则会计算$s = W_2max(0，W_1x)$。 这里，比如$W_1$可以是将图像变换成100维中间向量的[100×3072]矩阵。 函数$max(0， - )$是对每个元素计算的非线性函数（non-linearity that is applied elementwise）。 对于非线性函数（我们将在下面研究），我有几种不同的选择，$max(0， - )$是一个常见的选择，它只是将所有的激活阈值限制在0到0之间（作者笔误，我认为是0到x之间）。 最后，矩阵$W_2$的大小为[10x100]，所以我们再次得到10个数字，我们将其解释为类别分数。 请注意，非线性在计算上是非常重要的 - 如果我们将它排除在外，这两个矩阵可能会折叠成一个矩阵，因此预测的类别分数将再次成为输入的线性函数。 非线性函数是我们摆弄的地方（The non-linearity is where we get the wiggle.）。 参数$W_2$，$W_1$是用随机梯度下降（stochastic gradient descent）学习到的，它们的梯度是用链式规则（并用反向传播计算）推导出来的。

三层神经网络可以类似地看做$s = W_3max(0，W_2max(0，W_1x))$，其中$W_3$，$W_2$，$W_1$都是要学习的参数。 中间隐藏向量的长度是神经网络的超参数，如何设置它们我们以后再说。 现在让我们看看如何从神经元/网络的角度来解释这些计算。

## 对一个神经元建模
神经网络领域最初的主要灵感来源于为生物神经系统，但此后变为了工程问题，并在机器学习任务中取得了良好的结果。 尽管如此，我们仍用对带给神经网络领域很大启发的生物神经系统进行简短和高层次的描述，来开始我们的讨论。

### 生物性动力和连接
大脑的基本计算单位是一个神经元。人类神经系统中约860亿个可发现的**神经元（neuron）**，并且它们与大约$10 ^ {14}$ - $10 ^ {15}$个**突触（synapses）**相连。下图显示了生物神经元的图示（左）和常见的数学模型（右）。每个神经元接收来自其**树突（dendrites）**的输入信号并沿着其（单个）**轴突（axon）**产生输出信号。轴突最终分出并通过突触连接到其他神经元的树突。在神经元的计算模型中，沿轴突传播的信号（例如$x_0$）基于该突触（例如$w_0$）处的突触强度与另一神经元的树突相乘（例如$w_0x_0$）。突触强度（权重$w$）是可以学习的，并且控制一个神经元对另一个神经元的影响力（及其方向：兴奋（正面重量）或抑制（负面重量））。在基本模型中，树突将信号传送到细胞体，在那里它们都被求和。如果最终总和超过一定的阈值，神经元可以发射，沿轴突发出**冲动(spike)**。在计算模型中，我们假设冲动的准确时间不重要，只有发射的频率能传达信息。基于这种速率编码解释，我们用**激活函数$f$(activation function)**来模拟神经元的放电速率，该函数表示沿着轴突的冲动频率。从历史上看，激活函数的一个常见选择是**sigmoid函数$σ$**，因为它需要一个实值输入（即求和后的信号强度），并将其压缩至0和1之间。稍后在本节中我们将看到这些激活函数的细节。

> 左图：一个神经元的卡通图，右图：它的数学模型

单个神经元前向传播（forward-propagating）的示例代码如下所示：
```python
class Neuron(object):
  # ... 
  def forward(self, inputs):
    """ assume inputs and weights are 1-D numpy arrays and bias is a number """
    cell_body_sum = np.sum(inputs * self.weights) + self.bias
    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function
    return firing_rate
```

换句话说，每个神经元用输入及其权重执行点积，加上偏差并应用非线性函数（或激活函数），在上面的例子中，用的是sigmoid $σ(x)= 1 /(1 + e^{-x})$。 我们将在本节末尾详细介绍不同的激活函数。

**粗略模型**。 需要强调的是，这种生物神经元模型非常粗糙：例如，有许多不同类型的神经元，每种神经元都具有不同的属性。 生物神经元中的树突执行复杂的非线性计算。 突触不仅仅是一个单一的权重，他们是一个复杂的非线性动力系统。 已知许多系统中输出冲动的确切时间很重要，这表明速率代码近似可能不成立。 由于所有这些和其它许多简化，如果您在神经网络和真正的大脑之间进行类比，请准备好听取来自具有神经科学背景的任何人的质疑声。 如果您有兴趣，请参阅此[评论](https://physics.ucsd.edu/neurophysics/courses/physics_171/annurev.neuro.28.061604.135703.pdf)（pdf）或最近的[评论](https://www.sciencedirect.com/science/article/pii/S0959438814000130)。

### 单神经元作为线性分类器

神经元前向计算的数学形式可能看起来很熟悉。 正如我们用线性分类器所看到的那样，神经元有能力在输入空间的某些线性区域“喜欢”（激活接近1）或“不喜欢”（激活接近0）。 因此，通过对神经元输出的适当损失函数，我们可以将单个神经元变成线性分类器：

**二元Softmax分类器(Binary Softmax classifier)。** 例如，我们可以将$σ(Σ_iw_ix_i+ b)$解释为类$P(y_i = 1|x_i;w)$中的一个的概率。 另一个类的概率是$P(y_i = 0|x_i;w)= 1-P(y_i = 1|x_i;w)$，因为它们的和必须为1。 通过这种解释，我们可以制定在线性分类部分中看到的交叉熵损失（the cross-entropy loss），并且优化它成为二元Softmax分类器（也称为逻辑回归）。 由于S形函数被限制在0-1之间，因此该分类器的预测是基于神经元的输出是否大于0.5。

**二进制SVM分类器（Binary SVM classifier）**。 或者，我们可以在神经元的输出上附加一个最大边缘铰链损失（a max-margin hinge loss），并将其训练成二进制支持向量机（a binary Support Vector Machine）。

**正则化解释（Regularization interpretation）**。 在这个生物学视图中，SVM / Softmax情况下的正则化损失可以被解释为*逐渐遗忘（gradual forgetting）*，因为它会在每次更新参数后将所有突触权重w归到零。

> 可以使用单个神经元来实现二元分类器（例如，二元Softmax或二元SVM分类器）

### 常用的激活函数
每个激活函数（或非线性函数）都对一个数字执行某种固定的数学运算。 在实践中可能会遇到几种激活功能：


左：Sigmoid非线性将实数压缩到[0,1]之间的范围；右：tanh非线性将实数压缩到[-1,1]之间的范围。

**Sigmoid**。 S型非线性具有数学形式$σ(x)= 1 /(1 + e^{-x})$，并且在左上方的图像中示出。 如前所述，它取一个实数值并将其“压扁”到0到1之间。特别是，大的负数变为0，大的正数变为1。sigmoid函数在历史上经常使用 因为它具有很好的解释作为一个神经元的发射率：在假设的最大频率（1）下，从完全不发射（0）到完全饱和发射。 **实际上，S形非线性最近已经不受欢迎，并且很少被使用。 **它有两个主要缺点：

- Sigmoid饱和并杀死梯度（Sigmoids saturate and kill gradients）。 S形神经元非常不受欢迎的特性是，当神经元的激活在0或1的尾部饱和时，这些区域的梯度几乎为零。 回想一下，在反向传播过程中，这个（局部）梯度将乘以该节点对于整个目标函数输出的梯度。 因此，如果局部梯度非常小，它将有效地“杀死”梯度，几乎没有信号会通过神经元流向其权重并递归到其数据。 此外，在初始化S形神经元的权重以防止饱和时，必须格外小心。 例如，如果初始权重太大，那么大多数神经元会变得饱和，神经网络几乎不会学习。

- Sigmoid输出不是以0为中心的。 这是不可取的，因为在后面网络层中的神经元将接收不是以0为中心的数据。 这对梯度下降过程中的动力学有影响，因为如果进入神经元的数据总是正的（例如，在$f = w^Tx + b$中$x> 0$元素）），则反向传播期间权重w的梯度将全部为正面或全部负面（取决于整个表达式f的梯度）。 这会在权重的梯度更新中引入不希望的锯齿形动态(zig-zagging dynamics)。 但是，请注意，一旦这些梯度被累加到一批数据中，权重的最终更新可能会有可变的符号，这有点缓解了这个问题。 因此，这是一个不方便的地方，与上面的饱和激活问题相比，其后果不那么严重。

**tanh**。 tanh非线性函数显示在右上方的图像上。 它将实值数字压缩到范围[-1,1]。 像S形神经元一样，它的激活饱和，但与S形神经元不同的是，它的输出是零中心的。 因此，在实践中tanh非线性始终优于S形非线性。 还要注意tanh神经元只是一个缩放的S形神经元，特别是以下情况：$tanh(x)=2σ(2x)-1$。


左：整流线性单位（ReLU, Rectified Linear Unit）激活函数，当$x <0$时为0，当$x> 0$时为斜率1的线性关系。右：图（来自[Krizhevsky et al.](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)）显示与tanh单位相比，ReLU单位收敛速度提高了近6倍。

**ReLU**。整流线性单元在过去几年中变得非常流行。它的公式为$f(x)= max(0，x)$。换句话说，它只是简单地将阈值设置为零（参见左上图）。使用ReLUs有几个优点和缺点：

- （+）与S形/ tanh函数相比，随机梯度下降的收敛速度明显加快（例如[Krizhevsky](http://210.30.98.250/cache/1/03/www.cs.toronto.edu/afd1eea97d54a8e602e6bd883ad8faf1/imagenet.pdf)等人的6倍）。有人认为，这是由于其线性非饱和形式导致的。
- （+）与涉及昂贵操作（指数等）的tanh / sigmoid神经元相比，ReLU可以通过将激活矩阵简单地设置为零来实现。
- （ - ）不幸的是，在训练过程中，ReLU单位可能会变得脆弱，并可能“死亡”。例如，流经ReLU神经元的大梯度可能导致权重更新，使得神经元不会再次在任何数据点上激活。如果发生这种情况，那么从该点开始流经该单元的梯度将永远为零。也就是说，ReLU单位在训练期间可能会不可逆转地死亡，因为它们可能会从数据歧管（the data manifold）中被击倒。例如，如果学习率设置得太高，您可能会发现多达40％的网络节点“死亡”（即从未在整个训练数据集中激活的神经元）。通过适当设置学习率，这个问题就不那么常见了。

**Leaky ReLU**。Leaky ReLUs是解决“垂死的ReLU”问题的一种尝试。当$x <0$时，代替直接设为0的做法，Leaky ReLU将具有很小的负斜率（大约为0.01左右）。也就是说，函数计算$f（x）=𝟙(x <0)(αx)+𝟙(x> = 0)(x)$其中α是一个比较小的常数。有些人说用这种形式的激活函数是成功的，但结果并不总是一致的。负向区域的斜率也可以作为每个神经元的参数，正如2015年Kaiming He等人在[Delving Deep into Rectifiers](https://arxiv.org/abs/1502.01852)中介绍的PReLU神经元中所看到的那样。然而，跨任务的效益的一致性目前不清楚。

**MAXOUT**。还有其他类型的激活函数，其没有应用于$f(w^Tx + b)$上。 Maxout神经元（最近由Goodfellow等人介绍）是一种相对普遍的选择，它概括了ReLU及其Leaky版本。 Maxout神经元计算函数为$max(w^T_1x + b_1,w^T_2x + b_2)$。请注意，ReLU和Leaky ReLU都是这种形式的特例（例如，对于ReLU，我们有$w_1$，$b_1$ = 0）。 Maxout神经元因此享有ReLU单元的所有好处（线性操作状态，没有饱和），并且没有缺点（死亡的ReLU）。然而，与ReLU神经元不同，它使每个神经元的参数数量加倍，从而导致参数总数很高。

这就结束了我们对最常见类型的神经元及其激活函数的讨论。作为最后的评论，在同一网络中混合和匹配不同类型的神经元是非常罕见的，尽管这样做没有根本性的问题。

TLDR：**“我应该使用哪种神经元类型？”使用ReLU非线性，小心你的学习速率，并监测网络中可能“死亡”单元的比例。 如果你没法做到这一点，请尝试使用Leaky ReLU或Maxout。 切勿使用sigmoid。 试试tanh，但做好它比ReLU / Maxout更糟糕的打算。**

## 神经网络架构

### 网络层组织（Layer-wise organization）

**神经网络作为图中的神经元（Neural Networks as neurons in graphs）**。 神经网络被建模作为在非循环图中连接的神经元集合。 换句话说，一些神经元的输出可以成为其他神经元的输入。 循环是不允许的，因为这意味着在网络的正向传递中有无限循环。 神经网络模型通常组织成不同层次的神经元，而不是无组织的神经元块。 对于规则的神经网络，最常见的图层类型是**完全连接图层（fully-connected layer）**，其中两个相邻图层之间的神经元完全成对连接，但是单个图层内的神经元没有连接。 以下是两个使用完全连接图层的示例神经网络拓扑：

 
左图：2层神经网络（4个神经元（或单元）的隐藏层和2个神经元的输出层）和三个输入神经元。 右图：具有三个输入的三层神经网络，两个四个神经元的隐藏层和一个输出层。 请注意，在这两种情况下，跨层的神经元之间都存在连接（突触），但层内的神经元间不存在连接。

**命名约定(Naming conventions)。**请注意，当我们说N层神经网络时，我们不计入输入层。因此，单层神经网络描述了没有隐藏层的网络（输入直接映射到输出）。从这个意义上讲，你有时可以听到人们说逻辑回归或支持向量机只是单层神经网络的特例。您也可能会听到这些网络可互换地称为“人工神经网络”（ANN）或“多层感知器”（MLP）。许多人不喜欢神经网络和真实大脑之间的类比，而更喜欢将神经元作为单位。

**输出层（Output layer）。**与神经网络中的所有图层不同，输出层神经元通常不具有激活函数（或者您可以将它们视为具有线性的激活函数）。这是因为最后的输出层通常被用来表示类别分数（例如在分类中），其是任意的实数值，或者某种类型的实值目标（例如在回归中）。

**度量神经网络大小（Sizing neural networks）。**人们通常用来度量神经网络大小的两个指标是神经元的数量，或者更常见的是参数的数量。在上图中使用两个示例网络：

- 第一个网络（左）具有4 + 2 = 6个神经元（不包括输入），[3×4] + [4×2] = 20个权重和4 + 2 = 6个偏差，总共26个可学习参数。
- 第二个网络（右）具有4 + 4 + 1 = 9个神经元，[3×4] + [4×4] + [4×1] = 12 + 16 + 4 = 32个权重和4 + 4 + 1 = 9偏差，总共41个可学习参数。

介绍一些背景知识，现代卷积网络包含大约1亿个参数，通常由大约10-20层构成（因此深度学习）。但是，由于参数共享，我们将看到有效连接的数量明显增加。详看卷积神经网络模块。

### 前向计算示例
*重复的矩阵乘法与激活函数交织在一起。*神经网络组织为层的主要原因之一是这种结构使得使用矩阵向量操作来评估神经网络变得非常简单和高效。在上图中使用示例三层神经网络，输入将是[3x1]向量。一个图层的所有连接强度都可以存储在一个矩阵中。例如，第一个隐藏层的权重$W_1$的大小为[4x3]，所有单位的偏差都在向量$b_1$中，大小为[4x1]。在这里，每一个神经元都有一个$W_1$的权重，所以矩阵向量乘法`np.dot（W1，x）`计算该层所有神经元的激活。类似地，$W_2$将是一个[4x4]矩阵，用于存储第二个隐藏层的连接，$W_3$为最后一个（输出）层的[1x4]矩阵。这个3层神经网络的完全前向传递就是三个矩阵乘法，与激活函数的应用交织在一起：

```python
# forward-pass of a 3-layer neural network:
f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
x = np.random.randn(3, 1) # random input vector of three numbers (3x1)
h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)
h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)
out = np.dot(W3, h2) + b3 # output neuron (1x1)
```
在上面的代码中，W1，W2，W3，b1，b2，b3是网络的可学习参数。还要注意，变量x可以保存整批训练数据（其中每个输入示例将是x的一列），而不是具有单个输入列向量，然后所有示例将被一起有效地评估。请注意，最后的神经网络层通常不具有激活功能（例如，它表示分类设置中的（实值）类别分数）。

> 完全连接层的前向传播对应于一个矩阵乘法，随后是一个偏移(a bias offset)和激活函数。


### (Representational power)
一种用全连接层来观察神经网络的方法是，他们定义了一系列把网络权重作为参数的函数。自然出现的一个问题是：这个函数集的representational power是什么？特别是，是否有不能用神经网络建模的函数？

事实证明，至少有一个隐藏层的神经网络是**通用逼近器(universal approximators)**。也就是说，它可以表示出来（例如[1989年Sigmoidal函数叠加的逼近](http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf)（pdf），或[Michael Nielsen的这个直观的解释](http://neuralnetworksanddeeplearning.com/chap4.html)），给定任意连续函数$f(x)$和任意$ε> 0$时，存在一个神经网络$g(x)$带有一个隐藏层（具有合理的非线性选择，例如sigmoid），使得$∀x，| f(x)-g(x)| <ε$。换句话说，神经网络可以逼近任何连续函数。

如果一个隐藏层足以近似任何功能，为什么要使用更多的图层并更深入？答案是，双层神经网络是一个通用逼近器，尽管在数学上很让人喜欢，但在实践中却是相对较弱且无用的陈述。在一个维度中，“指标颠簸总和（sum of indicator bumps）”函数$g(x)=Σ_ic_i𝟙(a_i <x <b_i)$其中a，b，c是参数向量也是一个通用逼近器，但没有人会建议我们在机器学习中使用这个函数。神经网络在实践中运行良好，因为它们简介地表达了很好、平滑的函数，这些函数非常适合我们在实践中遇到数据的统计特性，并且使用我们的优化算法（例如梯度下降）也很容易学习。类似地，深层网络（具有多个隐藏层）可以比单隐层网络更好地工作的事实是一个经验观察，尽管它们的representational power是相等的。

顺便说一下，在实践中，3层神经网络通常会比2层网络性能更好，但更深层（4,5,6层）的帮助就很少。这与卷积网络形成鲜明对比，卷积网络已经发现深度对于良好识别系统（例如，按照10个可学习层的顺序）是非常重要的组成部分。这种观察的一个论据是，图像包含分层结构（例如，面部由眼睛组成，其由边缘等组成），因此多层处理对于该数据域而言是直观的。

当然，整个故事涉及更多，也是最近研究的主题。如果您对这些主题感兴趣，我们建议您进一步阅读：

- [Deep Learning](http://www.deeplearningbook.org/)， 本书由Bengio，Goodfellow，Courville出版，尤其是[第6.4章](http://www.deeplearningbook.org/contents/mlp.html)。
- [深度网络真的需要那么深入吗？](http://arxiv.org/abs/1312.6184)
- [FitNets：薄层深网的提示](https://arxiv.org/abs/1412.6550)

### 设置网络层数量及其大小
我们如何在遇到实际问题时决定使用哪种架构？我们应该使用隐藏层吗？一个隐藏层？两个隐藏层？每层应该有多大？首先，请注意，随着我们增加神经网络中的图层的大小和数量，网络的容量会增加。也就是说，可表示函数的空间会增加，因为神经元可以协作表达许多不同的功能。例如，假设我们在二维中存在二元分类问题。我们可以训练三个独立的神经网络，每个神经网络都有一个大小不等的隐藏层，并获得以下分类器：

较大的神经网络可以代表更复杂的功能。数据显示为按其类别上色的圆圈，并且由训练的神经网络决定的区域显示在下方。你可以在这个[ConvNetsJS演示](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)中玩这些例子。

在上图中，我们可以看到神经网络具有更多的神经元可以表达更复杂的功能。然而，这既是祝福（因为我们可以学习分类更复杂的数据）和诅咒（因为它更容易过度训练数据）。当具有高容量的模型符合数据中的噪音而不是（假定的）基础关系时，过度拟合就会发生。例如，具有20个隐藏神经元的模型适合于所有训练数据，但是以将空间分割成许多不相交的红色和绿色决策区域为代价。具有3个隐藏神经元的模型仅具有representational power来对广泛笔画中的数据进行分类。它将数据建模为两个斑点，并将绿色簇内的少数红色点解释为异常值（噪声）。在实践中，这可能会导致对测试集进行更好的泛化。

基于我们上面的讨论，如果数据不够复杂以防止过拟合，似乎较小的神经网络可能是优选的。然而，这是不正确的 - 有许多其他的优选方法可以防止我们将在后面讨论的神经网络中的过度拟合（例如L2正则化，丢失，输入噪声）。在实践中，使用这些方法来控制过度拟合，而不是神经元的数量多总是更好。

这背后的一个微妙的原因是，较小的网络更难以用梯度下降等局部方法进行训练：很显然，它们的损失函数具有相对较少的局部最小值，但事实证明，这些最小值中的许多更容易收敛，而且他们是坏的（即高损失）。相反，更大的神经网络包含更多的局部最小值，但是这些最小值在实际损失方面要好得多。由于神经网络是非凸的，所以很难从数学上研究这些性质，但是为了理解这些目标函数已经做了一些尝试，例如，在最近的论文多层网络的损失表面。在实践中，你发现如果你训练一个小型网络，最终的损失可能会表现出很大的差异 - 在某些情况下，你很幸运并且会聚到一个好的地方，但在某些情况下，你会陷入一个糟糕的最低限度。另一方面，如果你训练一个大型网络，你会开始发现许多不同的解决方案，但最终实现的损失差异会小得多。换句话说，所有的解决方案都大致相同，并且更少依赖于随机初始化的运气。

重申一下，正则化强度是控制神经网络过度拟合的首选方法。我们可以看看三种不同设置的结果：

正则化强度的影响：每个神经网络有20个隐藏的神经元，但改变正则化强度使其最终决策区域更平滑并具有更高的正则化。 你可以在这个[ConvNetsJS演示](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)中玩这些例子。

结论是你不应该使用较小的网络，因为你害怕过度配合。相反，您应该在计算能力允许下，使用尽量大的神经网络，然后使用其他正则化技术来控制过拟合。


## 概要
综上所述，
- 我们介绍了一个非常粗糙的生物神经元模型
- 我们讨论了在实践中使用的几种类型的激活函数，其中ReLU是最常见的选择
- 我们介绍了神经网络，其中神经元与完全连接的层连接，相邻层中的神经元具有完全的成对连接，但层内的神经元没有连接。
- 我们看到，这种分层架构能够基于与激活函数的应用交织的矩阵乘法，对神经网络进行高效评估。
- 我们看到，神经网络是通用函数逼近器，但我们也讨论了这个属性与它们无处不在的用途无关的事实。它们的使用是因为它们对实践中出现的函数的功能形式做出了某些“正确”的假设。
- 我们讨论了这样一个事实，即大型网络总是比小型网络工作得更好，但是它们的更高模型容量必须通过更强的正则化（比如更高的权重衰减）来适当解决，否则它们可能会过度适应。在后面的章节中我们会看到更多的正规化形式（特别是dropout）。

## 其他参考
- [deeplearning.net tutorial with Theano](http://www.deeplearning.net/tutorial/mlp.html)
- [ConvNetJS demos for intuitions](https://cs.stanford.edu/people/karpathy/convnetjs/)
- [Michael Nielsen’s tutorials](http://neuralnetworksanddeeplearning.com/chap1.html)


